# -*- coding: utf-8 -*-
"""BERT_FINE_TUNE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1atbYFK4JKC4uLnvueYKhZp830lJvu2Iz
"""
import random
import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments
import numpy as np
import logging
from typing import Dict
import argparse
from sklearn.metrics import precision_recall_fscore_support as score
import jsonlines


random.seed(10)

if torch.cuda.is_available():
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

def create_dataloader(tokenizer, file_path, batch_size, max_len, shuffle=True):
    labels, texts = [], []
    with jsonlines.open(file_path) as reader:
        for obj in reader:
            texts.append(obj['text'])
            labels.append(1 if obj['label'] == 'machine' else 0)
    
    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors="pt")
    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels))
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)

def data_collector(features):
    batch = {}
    batch['input_ids'] = torch.stack([f[0] for f in features])
    batch['attention_mask'] = torch.stack([f[1] for f in features])
    batch['labels'] = torch.stack([f[2] for f in features])
    
    return batch

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    acc = np.mean(preds == p.label_ids)
    precision, recall, fscore, support = score(p.label_ids.reshape(-1), preds, zero_division=0)

    metrics = {
        "accuracy": acc,
        "eval_acc": acc
    }

    # Handling single class output by checking the length of precision array
    if len(precision) > 1:
        metrics.update({
            "precision_human": precision[0],
            "recall_human": recall[0],
            "fscore_human": fscore[0],
            "support_human": float(support[0]),
            "precision_machine": precision[1],
            "recall_machine": recall[1],
            "fscore_machine": fscore[1],
            "support_machine": float(support[1])
        })
    else:
        # Assuming all predictions are of the one available class, which is 'machine'
        metrics.update({
            "precision_machine": precision[0],
            "recall_machine": recall[0],
            "fscore_machine": fscore[0],
            "support_machine": float(support[0])
        })

    return metrics


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--cache_dir", type=str, required=True)
    parser.add_argument("--prediction_output", type=str, required=True)
    parser.add_argument("--train_dir", type=str, required=True, help="Training dataset path")
    parser.add_argument("--val_dir", type=str, required=True, help="Validation dataset path")
    parser.add_argument("--test_dir", type=str, required=True, help="Test dataset path")
    parser.add_argument("--output_dir", type=str, required=True, help="Model checkpoints directory")
    parser.add_argument("--logging_file", type=str, required=True)
    parser.add_argument("--train_batch_size", type=int, required=True)
    parser.add_argument("--val_batch_size", type=int, required=True)
    parser.add_argument("--token_len", type=int, required=True, help="Token length for BERT")
    parser.add_argument("--model_ckpt_path", type=str, required=True, help="BERT model checkpoint")
    parser.add_argument("--num_train_epochs", type=int, required=True)
    parser.add_argument("--tensor_logging_dir", type=str, required=True)
    parser.add_argument("--save_steps", type=int, required=True)
    args = parser.parse_args()

    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir, exist_ok=True)

    print('Loading RoBERTa tokenizer...')
    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', cache_dir=args.cache_dir)

    train_loader = create_dataloader(tokenizer, args.train_dir, args.train_batch_size, args.token_len)
    val_loader = create_dataloader(tokenizer, args.val_dir, args.val_batch_size, args.token_len, shuffle=False)
    test_loader = create_dataloader(tokenizer, args.test_dir, args.val_batch_size, args.token_len, shuffle=False)

    model = RobertaForSequenceClassification.from_pretrained(args.model_ckpt_path, num_labels=2, cache_dir=args.cache_dir)

    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs!")

    model.to(device)
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        save_total_limit=2,
        overwrite_output_dir=True,
        do_train=True,
        do_eval=True,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=args.val_batch_size,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir=args.tensor_logging_dir,
        logging_steps=50,
        evaluation_strategy="steps",
        eval_steps=5,
        do_predict=True,
        metric_for_best_model="eval_acc",
        greater_is_better='True',
        save_steps=args.save_steps,
        load_best_model_at_end=True,
    )

    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
        filename=args.logging_file
    )

    logger = logging.getLogger(__name__)
    logger.info("Training/evaluation parameters %s", training_args)
    logger.info("Number of GPUs available: %s", torch.cuda.device_count())

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_loader.dataset,
        eval_dataset=val_loader.dataset,
        data_collator=data_collector,
        compute_metrics=compute_metrics
    )

    trainer.train()

    preds_val = trainer.predict(val_loader.dataset)
    preds_test = trainer.predict(test_loader.dataset)

    with jsonlines.open(args.prediction_output, 'w') as pred_out:
        pred_out.write(preds_val.metrics)
        pred_out.write(preds_test.metrics)

    print("Evaluation Metrics:", preds_val.metrics)
    print("Test Metrics:", preds_test.metrics)

if __name__ == "__main__":
    main()